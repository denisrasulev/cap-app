---
title: "How it works"
author: "Denis Rasulev"
date: "April, 2016"
output: html_document
---

## How it works

### Introduction
Here you will learn complete process from obtaining initial data to working app:   
1. Download and Import data
2. Clean data
3. Create Sample
4. Create Frequency Tables
5. Train Model or Save Tables?
6. Predict or Read Tables?
7. Shiny App

### Basic Preparation and Setup the Environment    
Here we load some libraries (we need only four :)) and setup working directory
```{r setup, eval=FALSE}
# load necessary R librabires
library(tm)         # Framework for text mining applications within R
library(NLP)        # Basic classes and methods for Natural Language Processing
library(slam)       # Data structures and algorithms for sparse arrays and matrices
library(ggplot2)    # Implementation of the grammar of graphics in R

# set our working directory
setwd("/Volumes/data/coursera/capstone")
```

### 3. Download and Import data
There is a link to download data from the internet. File contains texts for four languages. Out of four available (DE, US, FI, RU) we will use "US" training data for this project. We check if the initial file exists ("Coursera-SwiftKey.zip") and download / unzip it, if required.
```{r download, eval=FALSE}
# check if zip file already exists and download if it is missing
if (!file.exists("./data/Coursera-SwiftKey.zip")) {
    download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", 
    destfile = "./data/Coursera-SwiftKey.zip", method = "libcurl")
}

# check if data file exists and unzip it if necessary
if (!file.exists("./data/en_US/en_US.blogs.txt")) {
    unzip("Coursera-SwiftKey.zip", exdir = "./data/en_US", list = TRUE)
}
```

Then we make some functions that will help us to process the data. These are functions for text cleaning ('clean'), for counting terms frequencies and sorting them in decreasing order (sort.freq) and functions for tokenization (nXgram).

I would like to pay special attention to the cleaning functions that I have created. It took some time and numerous attempts to prepare the functions that gives more or less clean text at the end.

* First of all, we convert everything to lower case in order to avoid and duplicates.
* After that we remove all digits and control symbols.
* Then it took some time to prepare sort of dictionary to replace very common apostrophed phrases, like 'you're' -> 'you are'. In fact, all of these was manual work.
* Then we remove all web urls.
* Only after this we remove all punctuation signs! So yes, the order of what remove after what is important.
* After punctuation we replace all misspeled words like 'youre' -> 'you are' and they are of two types: those that can not be part of any word and those that can. So it required different coding to get them all correctly.
* Then it required to execute the very same code at least five times. Reason is simple - after removing digits there were lots of 'hanging' single letters. I didn't find better way to remove them at once, so did it five times. Seems to be working fine. Please, notice that single letter 'I' is not removed as it is important part of the language!
* Next, we do the same for the letters in the beginning of the sentences.
* Finally, we remove leading and trailing space.
* Done :)

One important finding. If you create relatively small corpus (<= 1GB) then base function 'rowSums' that is used to calculate frequencies of the terms works ok. As soon as you go for bigger numbers it generates errors. Therefore I have used 'row_sums' function from 'slam' package which was developed to work specifically with sparse matrices. And it works absolutely fine.

Another ineteresting finding was about library to use for tokenization. Majority of the people are using RWeka for these purposes. I have tested RWeka and NLP's package 'ngrams' function and have found that last one works TWICE as fast! Just for information, I am using MacBookPro with 2.8GHz Intel I7 and 16GB RAM. Probably on other platforms RWeka works better. But on MacOSX 'ngrams' allowed to reduce processing time two times.
```{r functions, eval=FALSE}
clean <- function(x)
{
    # convert everything to lower case
    x <- tolower(x)

    # remove numbers and control symbols
    x <- gsub("[[:digit:]]", "", x)
    x <- gsub("[[:cntrl:]]", "", x)

    # replace common apostrophed phrases
    x <- gsub("i'm",      "i am", x)
    x <- gsub("i've",     "i have", x)
    x <- gsub("it's",     "it is", x)
    x <- gsub("he's",     "he is", x)
    x <- gsub("isn't",    "is not", x)
    x <- gsub("let's",    "let us", x)
    x <- gsub("she's",    "she is", x)
    x <- gsub("i'll",     "i will", x)
    x <- gsub("you're",   "you are", x)
    x <- gsub("you'll",   "you will", x)
    x <- gsub("she'll",   "she will", x)
    x <- gsub("won't",    "will not", x)
    x <- gsub("we'll",    "we will", x)
    x <- gsub("he'll",    "he will", x)
    x <- gsub("it'll",    "it will", x)
    x <- gsub("can't",    "can not", x)
    x <- gsub("that's",   "that is", x)
    x <- gsub("thats",    "that is", x)
    x <- gsub("don't",    "do not", x)
    x <- gsub("didn't",   "did not", x)
    x <- gsub("wasn't",   "was not", x)
    x <- gsub("weren't",  "were not", x)
    x <- gsub("they'll",  "they will", x)
    x <- gsub("couldn't", "could not", x)
    x <- gsub("there's",  "there is", x)

    # remove web addresses and urls
    x <- gsub(" www(.+) ", "", x)
    x <- gsub(" http(.+) ", "", x)

    # remove punctuations marks
    x <- gsub("[[:punct:]]", "", x)

    # replace misspelled apostrohped phrases, not parts of any word
    x <- gsub("isnt",     "is not", x)
    x <- gsub("youre",    "you are", x)
    x <- gsub("itll",     "it will", x)
    x <- gsub("didnt",    "did not", x)
    x <- gsub("wasnt",    "was not", x)
    x <- gsub("youll",    "you will", x)
    x <- gsub("theyll",   "they will", x)
    x <- gsub("couldnt",  "could not", x)

    # replace misspelled apostrophed phrases, could be part of word
    x <- gsub("\\bim\\b",   "i am", x)
    x <- gsub("\\bive\\b",  "i have", x)
    x <- gsub("\\bdont\\b", "do not", x)
    x <- gsub("\\bcant\\b", "can not", x)
    x <- gsub("\\bwont\\b", "will not", x)
    x <- gsub("\\byouve\\b","you have", x)

    # remove remaining single letters (repeat 5 times)
    x <- gsub("\\b [a-hj-z]\\b", "", x)
    x <- gsub("\\b [a-hj-z]\\b", "", x)
    x <- gsub("\\b [a-hj-z]\\b", "", x)
    x <- gsub("\\b [a-hj-z]\\b", "", x)
    x <- gsub("\\b [a-hj-z]\\b", "", x)

    # remove single letters in the beginning of sentence
    x <- gsub("\\b[a-hj-z]\\b ", "", x)

    # remove leading and trailing spaces
    x <- gsub("^\\s+|\\s+$", "", x)

    return(x)
}

# function for sorting n-grams in decreasing order
sort.freq <- function(x){
    srt <- sort(row_sums(x, na.rm = T), decreasing = TRUE)
    frf <- data.frame(ngram = names(srt), freq = srt, row.names = NULL, check.rows = TRUE,  stringsAsFactors = FALSE)
    return(frf)
}

# functions for tokenization
n1gram <- function(x) unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
n2gram <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
n3gram <- function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
n4gram <- function(x) unlist(lapply(ngrams(words(x), 4), paste, collapse = " "), use.names = FALSE)
```

### 4. Exploratory Data Analysis
Let's see some basic statistics for 3 data files, including size, number of lines, words and characters, and words per line (WPL) summaries. We will also plot some histograms so we can see and better understand the distribution of these parameters.

From this statistics, we may see that WPL value for blogs is generally higher (41.75), followed by news (34.41) and twits (12.75). This reflects the nature of each communication channel and preferences of its' authors/readers.

From the histograms, we have also noticed that the WPL for all data types are right-skewed (i.e. have longer right tail). This may be an indication of the general trends towards shorter communications.

```{r basic stats, warning=FALSE}
# find files sizes
size.news   <- file.info("./data/en_US/en_US.news.txt")$size    / 1024^2
size.blogs  <- file.info("./data/en_US/en_US.blogs.txt")$size   / 1024^2
size.twits  <- file.info("./data/en_US/en_US.twitter.txt")$size / 1024^2

# count number of words
words.news  <- stri_count_words(news)
words.blogs <- stri_count_words(blogs)
words.twits <- stri_count_words(twits)

# count number of characters in words
chars.news  <- stri_stats_latex(news)["CharsWord"]
chars.blogs <- stri_stats_latex(blogs)["CharsWord"]
chars.twits <- stri_stats_latex(twits)["CharsWord"]

# prepare summary stats table
summary.table <- data.frame(File     = c("news", "blogs", "twitter"),
                            File.MB  = round(c(size.news, size.blogs, size.twits), 2),
                            N.Lines  = c(length(news), length(blogs), length(twits)),
                            N.Words  = c(sum(words.news), sum(words.blogs), sum(words.twits)),
                            N.Chars  = c(sum(chars.news), sum(chars.blogs), sum(chars.twits)),
                            WPL      = c(format(round(mean(words.news), 2), nsmall = 2),
                                         format(round(mean(words.blogs),2), nsmall = 2),
                                         format(round(mean(words.twits),2), nsmall = 2))
                            )
summary.table

# plot histograms for each data type
qplot(words.news,
        geom = "histogram",
        main = "Words per Publication for US News Texts",
        xlab = "Number of Words",
        ylab = "Frequency",
        xlim = c(0,200),
        binwidth = 2,
        fill = I("green"),
        col  = I("black"))

qplot(words.blogs,
        geom = "histogram",
        main = "Words per Publication for US Blogs Texts",
        xlab = "Number of Words",
        ylab = "Frequency",
        xlim = c(0,200),
        binwidth = 2,
        fill = I("green"),
        col  = I("black"))

qplot(words.twits,
        geom = "histogram",
        main = "Words per Publication for Twitter Texts",
        xlab = "Number of Words",
        ylab = "Frequency",
        xlim = c(0,200),
        binwidth = 2,
        fill = I("green"),
        col  = I("black"))

# remove unnecessary variables from memory
rm(chars.news, chars.blogs, chars.twits, size.news, size.blogs, size.twits, words.news, words.blogs, words.twits, summary.table)
```

### 5. Create Data Sample and Clean It
As our raw data is quite big, we will create smaller sample combining data from all 3 files (news, blogs, twits). After that we will clean it from everything that is not required or is not useful for our purposes, namely - digits, punctuation, control characters etc.

```{r sample}
# create sample
set.seed(2016)

# we'll take 5% of data from each file
sample.news  <- sample(news,  length(news)  * 0.05)
sample.blogs <- sample(blogs, length(blogs) * 0.05)
sample.twits <- sample(twits, length(twits) * 0.05)

# combine them into one
sample  <- c(sample.news, sample.blogs, sample.twits)

# prepare helper function
clean <- function(x)
{
    x <- tolower(x)
    x <- gsub("[[:digit:]]", "", x)
    x <- gsub("[[:punct:]]", "", x)
    x <- gsub("[[:cntrl:]]", "", x)
    x <- gsub("^\\s+|\\s+$", "", x)
    return(x)
}

# and clean our sample from unnecessary information
sample <- clean(sample)
```

### 6. Create Corpus and N-Grams
At this stage we will do the following:  
- Construct a corpus from the files.  
- Tokenization.  
- Build basic n-gram model.  

```{r corpus}
# helper function for convenience
freq <- function(x){
    srt <- sort(rowSums(as.matrix(x)), decreasing = TRUE)
    frf <- data.frame(word = names(srt), freq = srt)
    return(frf)
}

# prepare functions for tokenization
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
BeegramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))

# create our corpus
corp <- VCorpus(VectorSource(sample))

# clean corpus from profanity words
profanity  <- readLines("http://www.cs.cmu.edu/~biglou/resources/bad-words.txt")
corp <- tm_map(corp, removeWords, profanity)

# create term document matrix for unigrams
tdm1 <- TermDocumentMatrix(corp, control = list(tokenize = UnigramTokenizer))
tdm1 <- removeSparseTerms(tdm1, 0.99)
frq1 <- freq(tdm1)

# plot top 30 of unigrams
ggplot(frq1[1:30,],
        aes(x = reorder(word, freq), y = freq)) +
        geom_bar(stat = "identity", fill = "green", col = "black") +
        theme_bw() +
        coord_flip() +
        theme(axis.title.y = element_blank()) +
        labs(y = "Frequency", title = "Most common Unigrams in the Sample")

# create term document matrix for bigrams
tdm2 <- TermDocumentMatrix(corp, control = list(tokenize = BeegramTokenizer))
tdm2 <- removeSparseTerms(tdm2, 0.999)
frq2 <- freq(tdm2)

# plot top 30 of bigrams
ggplot(frq2[1:30,],
        aes(x = reorder(word, freq), y = freq)) +
        geom_bar(stat = "identity", fill = "green", col = "black") +
        theme_bw() +
        coord_flip() +
        theme(axis.title.y = element_blank()) +
        labs(y = "Frequency", title = "Most common Bigrams in the Sample")

# create term document matrix for trigrams
tdm3 <- TermDocumentMatrix(corp, control = list(tokenize = TrigramTokenizer))
tdm3 <- removeSparseTerms(tdm3, 0.9999)
frq3 <- freq(tdm3)

# plot top 30 of bigrams
ggplot(frq3[1:30,],
        aes(x = reorder(word, freq), y = freq)) +
        geom_bar(stat = "identity", fill = "green", col = "black") +
        theme_bw() +
        coord_flip() +
        theme(axis.title.y = element_blank()) +
        labs(y = "Frequency", title = "Most common Trigrams in the Sample")
```

### 7. Plans for the Project and App

Now that we have performed some exploratory analysis, we are ready to start building the predictive model(s) and eventually the data product. Below are high-level plan to achieve this goals:

- Use N-grams to generate tokens of one to four words.
- Find optimal balance between quality (sample size) and speed
- Build predictive model(s) using the tokens.
- Develop data product (Shiny app) to make next word prediction from user input.

For the Shiny, the plan is to create an app with a very simple interface where a user can enter a string of text. Prediction model will then give a list of suggested words to update the next one.
